# bigram language model 

>è¿™æ˜¯ä¸€ä¸ªå­—ç¬¦çº§ Bigram è¯­è¨€æ¨¡å‹, æ¥è‡ª Karpathy çš„æ•™å­¦ç¤ºä¾‹ã€‚åªæ ¹æ®å½“å‰å­—ç¬¦é¢„æµ‹ä¸‹ä¸€ä¸ªå­—ç¬¦ï¼Œæ²¡æœ‰ä¸Šä¸‹æ–‡å»ºæ¨¡ã€æ²¡æœ‰æ³¨æ„åŠ›ã€‚ä½†å®ƒå·²ç»å…·å¤‡å®Œæ•´ LLM è®­ç»ƒ & ç”Ÿæˆæµç¨‹

## 1. é¡¹ç›®ç›®æ ‡
- ä½¿ç”¨ **å­—ç¬¦çº§å»ºæ¨¡ï¼ˆcharacter-levelï¼‰**
    
- åŸºäº **Bigram å‡è®¾**ï¼š
    > ä¸‹ä¸€ä¸ªå­—ç¬¦åªä¾èµ–å½“å‰å­—ç¬¦
- å­¦ä¹ å†…å®¹ï¼š
    - æ•°æ®å‡†å¤‡
    - è¯è¡¨æ„å»º
    - æ¨¡å‹å®šä¹‰
    - è®­ç»ƒä¸è¯„ä¼°
    - æ–‡æœ¬ç”Ÿæˆ

## 2. è¶…å‚æ•°å®šä¹‰

```python
batch_size = 32      # æ¯ä¸ª batch çš„æ ·æœ¬æ•°
block_size = 8       # ä¸Šä¸‹æ–‡é•¿åº¦ï¼ˆTï¼‰
max_iters = 3000     # è®­ç»ƒè¿­ä»£æ¬¡æ•°
eval_interval = 300  # è¯„ä¼°é—´éš”
learning_rate = 1e-2
eval_iters = 200     # è¯„ä¼°æ—¶çš„ batch æ•°
device = 'cuda' if torch.cuda.is_available() else 'cpu'
torch.manual_seed(1337) # éšæœºç§å­
```


## 3. æ•°æ®è¯»å–

```python
with open('input.txt', 'r', encoding='utf-8') as f:
    text = f.read()
```

- `text` æ˜¯ä¸€ä¸ª**é•¿å­—ç¬¦ä¸²**
    
- æ•°æ®é›†ï¼šTiny Shakespeare


## 4. å­—ç¬¦çº§è¯è¡¨æ„å»º

### 4.1 è·å–æ‰€æœ‰å­—ç¬¦

```python
chars = sorted(list(set(text)))
vocab_size = len(chars)
```

- `chars`ï¼šæ‰€æœ‰å‡ºç°è¿‡çš„å­—ç¬¦
    
- `vocab_size`ï¼šå­—ç¬¦æ€»æ•°ï¼ˆâ‰ˆ 65ï¼‰

Tiny Shakespeare æ–‡æœ¬é‡Œå­—ç¬¦ä¸»è¦åŒ…æ‹¬ï¼š
1. **å°å†™å­—æ¯**ï¼š`a-z` â†’ 26 ä¸ª
2. **å¤§å†™å­—æ¯**ï¼š`A-Z` â†’ 26 ä¸ª
3. **æ ‡ç‚¹ç¬¦å·**ï¼š`, . ; : ' " ! ? -` ç­‰ â†’ 10 å·¦å³
4. **ç©ºæ ¼å’Œæ¢è¡Œ**ï¼šç©ºæ ¼ + `\n` â†’ 2
5. **å…¶ä»–ç¬¦å·**ï¼šå¦‚ `(`, `)` ç­‰ â†’ 1-2
    

---

### 4.2 ç¼–ç ä¸è§£ç æ˜ å°„

```python
stoi = {ch: i for i, ch in enumerate(chars)}
itos = {i: ch for i, ch in enumerate(chars)}
```

```python
encode = lambda s: [stoi[c] for c in s]
decode = lambda l: ''.join([itos[i] for i in l])
```

- **encode**ï¼šå­—ç¬¦ â†’ æ•´æ•°
    
- **decode**ï¼šæ•´æ•° â†’ å­—ç¬¦



---

## 5. æ„å»ºè®­ç»ƒ/éªŒè¯é›†

```python
data = torch.tensor(encode(text), dtype=torch.long)
```
- æ•´ä¸ªæ–‡æœ¬ â†’ æ•´æ•°åºåˆ—
```python
n = int(0.9 * len(data))
train_data = data[:n]
val_data = data[n:]
```
- 90% è®­ç»ƒ
- 10% éªŒè¯
    
## 6. batch é‡‡æ ·


```python
def get_batch(split):
```

> éšæœºä»æ–‡æœ¬ä¸­é‡‡æ ·ä¸€æ‰¹  
> `(x, y)` ç”¨äºè®­ç»ƒ

---


```python
data = train_data if split == 'train' else val_data
ix = torch.randint(len(data) - block_size, (batch_size,))
```

- `ix`ï¼šéšæœºèµ·å§‹ä½ç½®
    

```python
x = torch.stack([data[i:i+block_size] for i in ix])
y = torch.stack([data[i+1:i+block_size+1] for i in ix])
```

- `x`ï¼šè¾“å…¥å­—ç¬¦åºåˆ—
    
- `y`ï¼šç›®æ ‡å­—ç¬¦åºåˆ—ï¼ˆå³ç§»ä¸€ä½ï¼‰
    

```python
x, y = x.to(device), y.to(device)
return x, y
```

## 7. Bigram è¯­è¨€æ¨¡å‹å®šä¹‰


```python
class BigramLanguageModel(nn.Module):
```


```python
self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)
```

- è¾“å…¥ï¼šå½“å‰å­—ç¬¦ id
    
- è¾“å‡ºï¼šä¸‹ä¸€ä¸ªå­—ç¬¦çš„ logits
    
- ç­‰ä»·äºä¸€ä¸ªï¼š
    
    ```
    [vocab_size Ã— vocab_size] çš„æŸ¥è¡¨çŸ©é˜µ
    ```

- æ¯ä¸ªå­—ç¬¦ç›´æ¥å¯¹åº”ä¸€ä¸ª **é•¿åº¦ä¸º vocab_size çš„å‘é‡**
    
- è¿™ä¸ªå‘é‡å°±æ˜¯ **é¢„æµ‹ä¸‹ä¸€ä¸ªå­—ç¬¦çš„ logits**
    
- æ¢å¥è¯è¯´ï¼š
    

```text
self.token_embedding_table[i]  # i æ˜¯å½“å‰å­—ç¬¦çš„ id
```

- å¾—åˆ°çš„å‘é‡é•¿åº¦ = 65
    
- æ¯ä¸ªå…ƒç´ è¡¨ç¤º **é¢„æµ‹ä¸‹ä¸€ä¸ªå­—ç¬¦ä¸ºæŸä¸ªå­—ç¬¦çš„åˆ†æ•°**
    
- **è¿™ä¸ªå‘é‡ç›´æ¥å°±æ˜¯ softmax å‰çš„ logits**
    

æ‰€ä»¥ Bigram æ ¸å¿ƒå…¬å¼å°±æ˜¯ï¼š

```
P(next_char | current_char) = softmax(token_embedding_table[current_char])
```



---

### 7.1 forwardï¼ˆè®­ç»ƒï¼‰

```python
logits = self.token_embedding_table(idx)
```

- è¾“å…¥ï¼š`(B, T)`
    
- è¾“å‡ºï¼š`(B, T, vocab_size)`
    

```python
logits = logits.view(B*T, C)
targets = targets.view(B*T)
loss = F.cross_entropy(logits, targets)
```

---

### 7.2 generateï¼ˆæ–‡æœ¬ç”Ÿæˆï¼‰

```python
logits = logits[:, -1, :] # (B, C)
probs = F.softmax(logits, dim=-1) # (B, C)ï¼Œ è½¬åŒ–ä¸ºæ¦‚ç‡åˆ†å¸ƒ
idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)
```
  `torch.multinomial`ï¼š**ä»æ¦‚ç‡åˆ†å¸ƒä¸­éšæœºé‡‡æ ·**
    
- è¾“å…¥ï¼š`probs`ï¼Œæ¯è¡Œæ˜¯æ¦‚ç‡åˆ†å¸ƒ
- è¾“å‡ºï¼š`idx_next` â†’ ä¸‹ä¸€ä¸ªå­—ç¬¦çš„ **id**
- `num_samples=1` â†’ æ¯è¡Œé‡‡ä¸€ä¸ªæ ·æœ¬
- å½¢çŠ¶ï¼š`(B, 1)`
- åªçœ‹æœ€åä¸€ä¸ªå­—ç¬¦

```python
idx = torch.cat((idx, idx_next), dim=1)
```


ğŸ’¡ å°æŠ€å·§ï¼š

- å¯ä»¥ç”¨ `temperature` æ§åˆ¶ç”Ÿæˆéšæœºæ€§ï¼š
    

```python
temperature = 0.8
probs = F.softmax(logits / temperature, dim=-1)
idx_next = torch.multinomial(probs, 1)
```

- `temperature < 1` â†’ é€‰é«˜æ¦‚ç‡å­—ç¬¦æ›´å¤š â†’ æ–‡æœ¬æ›´â€œä¿å®ˆâ€
    
- `temperature > 1` â†’ ä½æ¦‚ç‡å­—ç¬¦ä¹Ÿä¼šè¢«é€‰ â†’ æ–‡æœ¬æ›´â€œå¤šæ ·åŒ–â€
    


## æ€»ä½“ä»£ç 

```python 
import torch
import torch.nn as nn
from torch.nn import functional as F

# hyperparameters
batch_size = 32 # ç‹¬ç«‹çš„å¥å­ï¼Œå¹¶è¡Œè®¡ç®—æ•°é‡
block_size = 8 #    ä¸Šä¸‹æ–‡é•¿åº¦
max_iters = 3000 #  è®­ç»ƒè¿­ä»£æ¬¡æ•°
eval_interval = 300 # è¯„ä¼°é—´éš”
learning_rate = 1e-2 # å­¦ä¹ ç‡
device = 'cuda' if torch.cuda.is_available() else 'cpu'
eval_iters = 200 # è¯„ä¼°æ—¶çš„batchæ•°
# ------------

torch.manual_seed(1337)

# wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt
with open('input.txt', 'r', encoding='utf-8') as f:
    text = f.read()


chars = sorted(list(set(text))) # æ‰€æœ‰å‡ºç°è¿‡çš„å­—ç¬¦
vocab_size = len(chars) # è¯è¡¨çš„é•¿åº¦ 65

stoi = { ch:i for i,ch in enumerate(chars) } # æ˜ å°„å­—å…¸ï¼Œ key: char, value: idx
itos = { i:ch for i,ch in enumerate(chars) } # æ˜ å°„å­—å…¸ï¼Œ key: idx, value: char
encode = lambda s: [stoi[c] for c in s] # encoder: ç»™ä¸€ä¸ªå­—ç¬¦ä¸²ï¼Œè¾“å‡ºå¯¹åº”ç´¢å¼•
decode = lambda l: ''.join([itos[i] for i in l]) # decoder: ç»™å®šç´¢å¼•ï¼Œè§£ç å­—ç¬¦ä¸²

# åˆ’åˆ†è®­ç»ƒå’ŒéªŒè¯é›† ï¼ˆ 9ï¼š1ï¼‰
data = torch.tensor(encode(text), dtype=torch.long)
n = int(0.9*len(data)) 
train_data = data[:n]
val_data = data[n:]

# æ•°æ®åŠ è½½
def get_batch(split):
    # generate a small batch of data of inputs x and targets y
    data = train_data if split == 'train' else val_data
    ix = torch.randint(len(data) - block_size, (batch_size,)) # éšæœºè·å–batch_sizeä¸ªèµ·å§‹å­—ç¬¦ç´¢å¼•
    x = torch.stack([data[i:i+block_size] for i in ix]) # xè¾“å…¥å­—ç¬¦åºåˆ—
    y = torch.stack([data[i+1:i+block_size+1] for i in ix]) # yï¼šç›®æ ‡å­—ç¬¦åºåˆ—ï¼ˆå³ç§»ä¸€ä½ï¼‰
    x, y = x.to(device), y.to(device)
    return x, y

@torch.no_grad()
def estimate_loss():
    out = {}
    model.eval()
    for split in ['train', 'val']:
        losses = torch.zeros(eval_iters)
        for k in range(eval_iters):
            X, Y = get_batch(split)
            logits, loss = model(X, Y)
            losses[k] = loss.item()
        out[split] = losses.mean()
    model.train()
    return out

# super simple bigram model
class BigramLanguageModel(nn.Module):

    def __init__(self, vocab_size):
        super().__init__()
        # each token directly reads off the logits for the next token from a lookup table
        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)

    def forward(self, idx, targets=None):

        # idx and targets are both (B,T) tensor of integers
        logits = self.token_embedding_table(idx) # (B,T,C)

        if targets is None:
            loss = None
        else:
            B, T, C = logits.shape
            logits = logits.view(B*T, C)
            targets = targets.view(B*T)
            loss = F.cross_entropy(logits, targets)

        return logits, loss

    def generate(self, idx, max_new_tokens):
        # idxï¼šå½“å‰ä¸Šä¸‹æ–‡å­—ç¬¦åºåˆ—ï¼ˆæ•´æ•°ç´¢å¼•ï¼‰ï¼Œå½¢çŠ¶ (B, T)
        for _ in range(max_new_tokens):
            # get the predictions, è°ƒç”¨forward
            logits, loss = self(idx)
            # å–æœ€åä¸€ä¸ªæ—¶é—´æ­¥ï¼Œå› ä¸º Bigram æ¨¡å‹åªä¾èµ–å½“å‰æœ€åä¸€ä¸ªå­—ç¬¦
            logits = logits[:, -1, :] # becomes (B, C)
            # è½¬æˆæ¦‚ç‡åˆ†å¸ƒ
            probs = F.softmax(logits, dim=-1) # (B, C)
            # æ ¹æ®æ¦‚ç‡é‡‡æ ·ä¸‹ä¸€ä¸ªå­—ç¬¦
            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)
            # æ‹¼æ¥åˆ°ç°æœ‰åºåˆ—
            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)
        return idx # å½¢çŠ¶ (B, T + max_new_tokens)

model = BigramLanguageModel(vocab_size)
m = model.to(device)

# create a PyTorch optimizer
optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)

for iter in range(max_iters):

    # æ¯ eval_interval = 300 æ­¥ï¼Œè¯„ä¼°è®­ç»ƒé›†å’ŒéªŒè¯é›†çš„å¹³å‡ loss
    # estimate_loss() æ˜¯å‰é¢å®šä¹‰çš„æ— æ¢¯åº¦å‡½æ•°
    if iter % eval_interval == 0:
        losses = estimate_loss()
        print(f"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}")

    # è·å–ä¸€ä¸ªbatchçš„æ•°æ®
    xb, yb = get_batch('train')

    # å‰å‘ + loss
    logits, loss = model(xb, yb)
    # æ¸…é™¤ä¸Šä¸€æ­¥æ¢¯åº¦
    optimizer.zero_grad(set_to_none=True)
    # åå‘ä¼ æ’­
    loss.backward()
    # æ¢¯åº¦æ›´æ–°
    optimizer.step()

# generate from the model
context = torch.zeros((1, 1), dtype=torch.long, device=device)
print(decode(m.generate(context, max_new_tokens=500)[0].tolist()))


```

## ç»“æœ

```python
(yolo) xujg@xujg-ASUS:~/code/ng-video-lecture$ python bigram.py 
step 0: train loss 4.7305, val loss 4.7241
step 300: train loss 2.8110, val loss 2.8249
step 600: train loss 2.5434, val loss 2.5682
step 900: train loss 2.4932, val loss 2.5088
step 1200: train loss 2.4863, val loss 2.5035
step 1500: train loss 2.4665, val loss 2.4921
step 1800: train loss 2.4683, val loss 2.4936
step 2100: train loss 2.4696, val loss 2.4846
step 2400: train loss 2.4638, val loss 2.4879
step 2700: train loss 2.4738, val loss 2.4911



CEThik brid owindakis b, bth

HAPet bobe d e.
S:
O:3 my d?
LUCous:
Wanthar u qur, t.
War dXENDoate awice my.

Hastarom oroup
Yowhthetof isth ble mil ndill, ath iree sengmin lat Heriliovets, and Win nghir.
Swanousel lind me l.
HAshe ce hiry:
Supr aisspllw y.
Hentofu n Boopetelaves
MPOLI s, d mothakleo Windo whth eisbyo the m dourive we higend t so mower; te

AN ad nterupt f s ar igr t m:

Thin maleronth,
Mad
RD:

WISo myrangoube!
KENob&y, wardsal thes ghesthinin couk ay aney IOUSts I&fr y ce.
J
```