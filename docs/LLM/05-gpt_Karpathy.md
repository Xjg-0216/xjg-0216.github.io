# gpt

>æ¥è‡ªkarpathy Lets build GPT_ from scratch, in code, spelled outè¯¾ç¨‹ï¼Œè®­ç»ƒå­—ç¬¦çº§gpt,éƒ¨åˆ†æ­¥éª¤ä¸bigramç›¸åŒ


## 1. è¶…å‚æ•°å®šä¹‰

```python
batch_size = 64  # å¹¶è¡Œè®­ç»ƒå¤šå°‘æ®µæ–‡æœ¬
block_size = 256 # æœ€å¤§ä¸Šä¸‹æ–‡é•¿åº¦
n_embd = 384 # token è¡¨ç¤ºç»´åº¦
n_head = 6 # è‡ªæ³¨æ„åŠ›å¤´
n_layer = 6 # transformerçš„å±‚æ•°
dropout = 0.2 # æ­£åˆ™åŒ–å‚æ•°
```

> å­—ç¬¦çº§è¯è¡¨æ„å»ºï¼Œæ„å»ºè®­ç»ƒã€éªŒè¯é›†ã€batché‡‡ç”¨å‡ä¸ä¸ŠèŠ‚å†…å®¹ç›¸åŒ



## 2. æ€»ä½“è¿‡ç¨‹

```text
input idx
 â†“
Token Embedding + Position Embedding
 â†“
[ Transformer Block Ã— 6 ]
 â†“
LayerNorm
 â†“
Linear â†’ vocab_size
 â†“
CrossEntropyLoss
```

## 3. Token Embedding

```python
self.token_embedding_table = nn.Embedding(vocab_size, n_embd)

```

`Token Embedding Table`æ˜¯ä¸€ä¸ªå¯å­¦ä¹ çš„æŸ¥è¡¨çŸ©é˜µ
- æ¯ä¸€è¡Œï¼šä¸€ä¸ª tokenï¼ˆå­—ç¬¦ï¼‰
- æ¯ä¸€åˆ—ï¼šè¯¥ token åœ¨æŸä¸ªè¯­ä¹‰ç»´åº¦ä¸Šçš„åˆ†é‡

åœ¨æ­¤ä»£ç ä¸­ä¸€å…±æœ‰65ä¸ª`token`ï¼Œ ä¹Ÿå°±æ˜¯`vocab_size`çš„é•¿åº¦ï¼Œ `n_embd`æ˜¯è¯­ä¹‰ç»´ä¸º384,æ¯ä¸€ä¸ª token è¢«æ˜ å°„æˆä¸€ä¸ª 384 ç»´å‘é‡

**ä¸Bigram embeddingdeä¸åŒ**

Bigram ä¸­çš„ embedding

`nn.Embedding(vocab_size, vocab_size)`

- è¾“å…¥ token id
- ç›´æ¥è¾“å‡º **ä¸‹ä¸€ä¸ª token çš„ logits**
    
`token i â†’ [P(next=0), P(next=1), ..., P(next=64)]`

ğŸ‘‰ **è¿™æ˜¯ä¸€ä¸ªâ€œæŸ¥æ¦‚ç‡è¡¨â€çš„æ¨¡å‹**
GPT ä¸­çš„ embedding

`nn.Embedding(vocab_size, n_embd)`

- token â†’ **è¯­ä¹‰è¡¨ç¤ºå‘é‡**
    
- ä¸ç›´æ¥é¢„æµ‹
    
- åé¢è¿˜è¦ç»è¿‡ **Attention + FFN**
    

ğŸ‘‰ **è¿™æ˜¯â€œç‰¹å¾è¡¨ç¤ºâ€ï¼Œä¸æ˜¯æ¦‚ç‡**


## 4. Position Embedding

```python
self.position_embedding_table = nn.Embedding(block_size, n_embd)
```
```text
(block_size, n_embd) = (256, 384)
```

- æ¯ä¸€è¡Œ = åºåˆ—ä¸­çš„ä¸€ä¸ª **ä½ç½®**
    
- ç¬¬ 0 ä¸ª tokenã€ç¬¬ 1 ä¸ª tokenã€ç¬¬ 2 ä¸ª tokenâ€¦â€¦


**ä¸ºä»€ä¹ˆPosition Embedding**


> **Attention æœ¬èº«ä¸æ„ŸçŸ¥é¡ºåº**

```text
"ABC" å’Œ "CBA" â†’ Attention çœ‹åˆ°çš„æ˜¯åŒä¸€ç»„ token
```

```python
x = tok_emb + pos_emb
```

è¿™ä¸€æ­¥è®©æ¨¡å‹çŸ¥é“ï¼š

> â€œè¿™æ˜¯ç¬¬å‡ ä¸ª tokenâ€

## 5. GPTLanguageModelæ¨¡å‹ç»“æ„

`Head`ï¼š**å•å¤´è‡ªæ³¨æ„åŠ›ï¼ˆSelf-Attention Headï¼‰**


```python
class Head(nn.Module):
    """ one head of self-attention """
```

è¡¨ç¤º **ä¸€ä¸ª attention head**
GPT ä¸­é€šå¸¸æ˜¯ **å¤šå¤´æ³¨æ„åŠ› = å¤šä¸ª Head å¹¶è¡Œ**

---

åˆå§‹åŒ–å‡½æ•°

```python
def __init__(self, head_size):
    super().__init__()
```

`head_size`ï¼š

* æ¯ä¸ª head çš„ç»´åº¦
* é€šå¸¸ï¼š`head_size = n_embd // n_head`

---

Key / Query / Value çº¿æ€§å±‚

```python
self.key = nn.Linear(n_embd, head_size, bias=False)
self.query = nn.Linear(n_embd, head_size, bias=False)
self.value = nn.Linear(n_embd, head_size, bias=False)
```

> æ³¨æ„ï¼š
>
> * **GPT æ˜¯ self-attention** â†’ Qã€Kã€V éƒ½æ¥è‡ªåŒä¸€ä¸ª `x`
> * `bias=False` æ˜¯æ ‡å‡† Transformer è®¾ç½®


ä¸‹ä¸‰è§’ maskï¼ˆå› æœæ©ç ï¼‰

```python
self.register_buffer(
    'tril',
    torch.tril(torch.ones(block_size, block_size))
)
```

ä½œç”¨ï¼š**é˜²æ­¢â€œå·çœ‹æœªæ¥â€**

ä¸¾ä¾‹ï¼ˆT=5ï¼‰ï¼š

```
1 0 0 0 0
1 1 0 0 0
1 1 1 0 0
1 1 1 1 0
1 1 1 1 1
```

è¡¨ç¤ºï¼š

* ç¬¬ t ä¸ª token **åªèƒ½çœ‹åˆ° â‰¤ t çš„ token**
* GPT æ˜¯ **è‡ªå›å½’è¯­è¨€æ¨¡å‹ï¼ˆcausal LMï¼‰**

ğŸ“Œ `register_buffer`ï¼š

* ä¸å‚ä¸è®­ç»ƒ
* ä¼šè‡ªåŠ¨ `.to(device)`
* ä¼šè¢«ä¿å­˜è¿› `state_dict`



Dropout

```python
self.dropout = nn.Dropout(dropout)
```

é˜²æ­¢ attention è¿‡æ‹Ÿåˆ

---

forward è¿‡ç¨‹ï¼ˆé‡ç‚¹ï¼‰

```python
def forward(self, x):
    B, T, C = x.shape
```

* Bï¼šbatch size
* Tï¼šåºåˆ—é•¿åº¦ï¼ˆtime stepsï¼‰
* Cï¼šembedding dim


è®¡ç®— Qã€K

```python
k = self.key(x)   # (B, T, hs)
q = self.query(x) # (B, T, hs)
```


Attention æƒé‡è®¡ç®—ï¼ˆæ ¸å¿ƒå…¬å¼ï¼‰

```python
wei = q @ k.transpose(-2, -1) * k.shape[-1]**-0.5
```

$$
\text{Attention}(Q, K, V)
= \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V
$$

ç»´åº¦å˜åŒ–ï¼š

```
(B, T, hs) @ (B, hs, T) â†’ (B, T, T)
```

æ¯ä¸ª token å¯¹ **æ‰€æœ‰ token** çš„ç›¸ä¼¼åº¦


Causal Mask

```python
wei = wei.masked_fill(
    self.tril[:T, :T] == 0,
    float('-inf')
)
```

* æŠŠæœªæ¥ä½ç½®è®¾ä¸º `-inf`
* softmax åæ¦‚ç‡ä¸º 0

Softmax

```python
wei = F.softmax(wei, dim=-1)
```

å¾—åˆ°ï¼š

```
æ¯ä¸ª token å¯¹è¿‡å» token çš„æ³¨æ„åŠ›åˆ†å¸ƒ
```

Dropout

```python
wei = self.dropout(wei)
```
åŠ æƒæ±‚å’Œï¼ˆValueï¼‰

```python
v = self.value(x)     # (B, T, hs)
out = wei @ v         # (B, T, hs)
```

æ¯ä¸ª token = æ‰€æœ‰ token çš„ value åŠ æƒå’Œ

---

`MultiHeadAttention`ï¼š**å¤šå¤´æ³¨æ„åŠ›**


åˆå§‹åŒ–

```python
self.heads = nn.ModuleList(
    [Head(head_size) for _ in range(num_heads)]
)
```

* å¹¶è¡Œå¤šä¸ª Head
* æ¯ä¸ª head å…³æ³¨ä¸åŒæ¨¡å¼

è¾“å‡ºæ˜ å°„

```python
self.proj = nn.Linear(head_size * num_heads, n_embd)
```

æŠŠæ‹¼æ¥åçš„ç»“æœæ˜ å°„å› `n_embd`


forward

```python
out = torch.cat([h(x) for h in self.heads], dim=-1)
```

ç»´åº¦ï¼š

```
num_heads Ã— (B, T, hs) â†’ (B, T, n_embd)
```


```python
out = self.dropout(self.proj(out))
```
==å•å¤´æ³¨æ„åŠ›å’Œå¤šå¤´æ³¨æ„åŠ›å…¬å¼æ¨å¯¼==


å•å¤´æ³¨æ„åŠ›ï¼ˆSingle-Head Attentionï¼‰



ç»™å®šè¾“å…¥ $(X \in \mathbb{R}^{L \times d_{\text{model}}})$ ï¼š

$$
\begin{aligned}
Q &= XW_Q \\
K &= XW_K \\
V &= XW_V
\end{aligned}
$$

ç„¶åè®¡ç®— **ç¼©æ”¾ç‚¹ç§¯æ³¨æ„åŠ›**ï¼š

$$
\text{Attention}(Q, K, V)
= \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V
$$

è¾“å‡ºç»´åº¦ä»ç„¶æ˜¯ï¼š

$$
L \times d_v
$$


å¤šå¤´æ³¨æ„åŠ›ï¼ˆMulti-Head Attentionï¼‰

è®¡ç®—æ–¹å¼

å‡è®¾æœ‰ $h$ ä¸ªå¤´ï¼ˆheadï¼‰ï¼š

ï¼ˆ1ï¼‰çº¿æ€§æŠ•å½±ï¼ˆæ¯ä¸ªå¤´ä¸€å¥—å‚æ•°ï¼‰

$$
\begin{aligned}
Q_i &= XW_Q^{(i)} \
K_i &= XW_K^{(i)} \
V_i &= XW_V^{(i)}
\end{aligned}
\quad i = 1,2,\dots,h
$$

é€šå¸¸ï¼š
$$
d_k = d_v = \frac{d_{\text{model}}}{h}
$$

ï¼ˆ2ï¼‰æ¯ä¸ªå¤´ç‹¬ç«‹ç®—æ³¨æ„åŠ›

$$
\text{head}_i
= \text{Attention}(Q_i, K_i, V_i)
$$


ï¼ˆ3ï¼‰æ‹¼æ¥ + çº¿æ€§å˜æ¢

$$
\text{MultiHead}(Q,K,V)
= \text{Concat}(\text{head}_1,\dots,\text{head}_h)W_O
$$

è¾“å‡ºç»´åº¦ä»æ˜¯ï¼š
$$
L \times d_{\text{model}}
$$


---
FeedForward`ï¼š**é€ token çš„ MLP**

> æ³¨æ„ï¼š**ä¸åš token é—´é€šä¿¡**
> åªæ˜¯å¯¹æ¯ä¸ª token ç‹¬ç«‹åšéçº¿æ€§å˜æ¢


ç»“æ„

```python
nn.Linear(n_embd, 4 * n_embd)
nn.ReLU()
nn.Linear(4 * n_embd, n_embd)
```

è¿™æ˜¯ GPT ç»å…¸ç»“æ„ï¼š

* **å‡ç»´ â†’ éçº¿æ€§ â†’ é™ç»´**
* å¢å¼ºè¡¨è¾¾èƒ½åŠ›



forward

```python
return self.net(x)
```

Block`ï¼š**å®Œæ•´ Transformer Block**

> Transformer =
> **Attentionï¼ˆé€šä¿¡ï¼‰ + FFNï¼ˆè®¡ç®—ï¼‰**

---

åˆå§‹åŒ–

```python
head_size = n_embd // n_head
self.sa = MultiHeadAttention(n_head, head_size)
self.ffwd = FeedFoward(n_embd)
```
LayerNormï¼ˆPre-LNï¼‰

```python
self.ln1 = nn.LayerNorm(n_embd)
self.ln2 = nn.LayerNorm(n_embd)
```

âš ï¸ GPT ä½¿ç”¨ **Pre-LN**ï¼ˆæ¯” Post-LN ç¨³å®šï¼‰

forwardï¼ˆé‡ç‚¹ï¼‰

```python
x = x + self.sa(self.ln1(x))
```

1. LayerNorm
2. Self-Attention
3. **Residualï¼ˆæ®‹å·®è¿æ¥ï¼‰**

```python
x = x + self.ffwd(self.ln2(x))
```

1. LayerNorm
2. FeedForward
3. Residual


æ•´ä½“æ•°æ®æµ

```
Token Embedding + Position Embedding
        â†“
[ Block 1 ]
    LN â†’ Multi-Head Attention â†’ +res
    LN â†’ FeedForward        â†’ +res
        â†“
[ Block 2 ]
        â†“
...
        â†“
LayerNorm
Linear â†’ vocab_size
```

---




### 5.1 Token Embeddingï¼ˆå­—ç¬¦ â†’ å‘é‡ï¼‰

```python
self.token_embedding_table = nn.Embedding(vocab_size, n_embd)
```

**ä½œç”¨**

* æŠŠ **ç¦»æ•£çš„ token id** æ˜ å°„ä¸º **è¿ç»­å‘é‡**
* æ¯ä¸ª token å¯¹åº”ä¸€ä¸ª `n_embd` ç»´å‘é‡

**å‚æ•°å½¢çŠ¶**

```text
(vocab_size, n_embd)
```


### 5.2 Position Embeddingï¼ˆä½ç½® â†’ å‘é‡ï¼‰

```python
self.position_embedding_table = nn.Embedding(block_size, n_embd)
```

**ä½œç”¨**

* ç»™ Transformer æ³¨å…¥ **åºåˆ—é¡ºåºä¿¡æ¯**
* ç¬¬ 0 ä¸ª tokenã€ç¬¬ 1 ä¸ª tokenâ€¦â€¦éƒ½æœ‰ç‹¬ç«‹è¡¨ç¤º

**å‚æ•°å½¢çŠ¶**

```text
(block_size, n_embd)
```


### 5.3 Transformer Blocks

```python
self.blocks = nn.Sequential(
    *[Block(n_embd, n_head=n_head) for _ in range(n_layer)]
)
```

* å †å  `n_layer` ä¸ª Transformer Block
* æ¯ä¸ª Block åŒ…å«ï¼š

  * Multi-Head Self-Attention
  * FeedForward
  * Residual + LayerNorm


### 5.4 æœ€åçš„ LayerNorm

```python
self.ln_f = nn.LayerNorm(n_embd)
```

**ä½œç”¨**

* åœ¨è¾“å‡º logits å‰ç¨³å®šç‰¹å¾åˆ†å¸ƒ
* GPT çš„æ ‡å‡†åšæ³•ï¼ˆPre-LN æ¶æ„ï¼‰


### 5.5 Language Model Headï¼ˆå‘è¯è¡¨æŠ•å½±ï¼‰

```python
self.lm_head = nn.Linear(n_embd, vocab_size)
```

**ä½œç”¨**

* æŠŠéšè—å‘é‡æ˜ å°„æˆ **ä¸‹ä¸€ä¸ª token çš„ logits**
* æ¯ä¸ªä½ç½®éƒ½æ˜¯ä¸€ä¸ª `vocab_size` åˆ†ç±»é—®é¢˜

**è¾“å‡ºå½¢çŠ¶**

```text
(B, T, vocab_size)
```

### 5.6ï¸ æƒé‡åˆå§‹åŒ–

```python
self.apply(self._init_weights)
```

* å¯¹æ¨¡å‹ä¸­æ‰€æœ‰å­æ¨¡å—é€’å½’è°ƒç”¨ `_init_weights`
* ä½¿ç”¨ GPT å¸¸ç”¨åˆå§‹åŒ–æ–¹å¼



**Linear å±‚åˆå§‹åŒ–**

```python
if isinstance(module, nn.Linear):
    torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)
```

* æƒé‡ï¼šé«˜æ–¯åˆ†å¸ƒ
* æ ‡å‡†å·® `0.02`ï¼ˆGPT è®ºæ–‡ç»éªŒå€¼ï¼‰

```python
if module.bias is not None:
    torch.nn.init.zeros_(module.bias)
```

* bias å…¨ 0


**Embedding åˆå§‹åŒ–**

```python
elif isinstance(module, nn.Embedding):
    torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)
```

* token embedding å’Œ position embedding éƒ½ç”¨åŒæ ·è§„åˆ™


### 5.7 Transformer Blocks

```python
x = self.blocks(x)
```

* å¤šå±‚ Attention + FFN
* èåˆå†å²ä¸Šä¸‹æ–‡ä¿¡æ¯

**å½¢çŠ¶ä¿æŒä¸å˜**

```text
(B, T, n_embd)
```


### 5.8 `generate`ï¼šè‡ªå›å½’æ–‡æœ¬ç”Ÿæˆ

```python
def generate(self, idx, max_new_tokens):
```
**è¾“å…¥**

* `idx`: å½“å‰ä¸Šä¸‹æ–‡ `(B, T)`
* `max_new_tokens`: è¦ç”Ÿæˆå¤šå°‘ token


**å¾ªç¯ç”Ÿæˆ**

```python
for _ in range(max_new_tokens):
```


**æˆªæ–­ä¸Šä¸‹æ–‡**

```python
idx_cond = idx[:, -block_size:]
```

* åªä¿ç•™æœ€è¿‘ `block_size` ä¸ª token
* é˜²æ­¢ä½ç½® embedding è¶Šç•Œ


**å‰å‘é¢„æµ‹**

```python
logits, loss = self(idx_cond)
```
**å–æœ€åä¸€ä¸ªæ—¶é—´æ­¥**

```python
logits = logits[:, -1, :]
```

**å½¢çŠ¶**

```text
(B, vocab_size)
```

> ç”¨å½“å‰ä¸Šä¸‹æ–‡é¢„æµ‹ä¸‹ä¸€ä¸ª token


**Softmax â†’ æ¦‚ç‡**

```python
probs = F.softmax(logits, dim=-1)
```

---

**é‡‡æ ·ä¸‹ä¸€ä¸ª token**

```python
idx_next = torch.multinomial(probs, num_samples=1)
```

**è¾“å‡º**

```text
(B, 1)
```
**æ‹¼æ¥åˆ°åºåˆ—æœ«å°¾**

```python
idx = torch.cat((idx, idx_next), dim=1)
```
