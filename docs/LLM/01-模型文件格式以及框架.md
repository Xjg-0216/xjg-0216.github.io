# 模型文件格式以及框架


在 大模型（LLM、生成模型）领域，模型文件本身主要有几种常见格式，每种格式适用于不同框架、推理引擎或硬件平台


## llama.cpp


> 💡**底层轻量推理引擎**，专为 **CPU 推理** 设计，非常小巧。


| 描述   | 内容                            |
| ---- | ----------------------------- |
| 本质   | C/C++ 实现的 **LLM 推理核心**        |
| 模型格式 | **GGUF 格式**                   |
| 硬件支持 | CPU（也支持 GPU/Metal/CUDA 但效果一般） |
| 依赖   | 几乎没有                          |
| 适合人群 | 喜欢极致轻量、能自己写命令的人               |

**适用场景**

✔️ 嵌入式设备（如 **树莓派、开发板**）
✔️ 低配置电脑（仅CPU）
✔️ 需要 **本地隐私计算**
❌ 不适合高并发或AI Agent场景

---

## **Ollama**

> 💡**基于 llama.cpp 封装的高易用性 LLM 管理与对话工具**
> 可理解为 **llama.cpp 的 UI + 模型管理系统 + Agent 框架**


| 描述       | 内容                                                     |
| -------- | ------------------------------------------------------ |
| 底层       | llama.cpp                                              |
| 模型格式     | `.gguf`（推荐）                                            |
| 使用方式     | `ollama run llama3`                                    |
| 支持 Agent | ✔ 自动支持 **API / function calling / prompt engineering** |
| 适用硬件     | CPU / GPU 都支持                                          |
| 开发难度     | 极低，直接用                                                 |

**适用场景**

✔️ **本地对话助手（ChatGPT 替代）**
✔️ **自动 Agent（可调用你的函数）**
✔️ 想快速体验大模型（无需配置环境）
✔️ 和你的设备互联时作为 LLM 控制模块
❌ 不适合高吞吐服务器推理

---

## **vLLM**

> 💡**高性能 GPU 推理框架（服务器级）**
> 优化了 GPU 显存管理和推理吞吐，适合部署企业 LLM 服务（如 OpenAI API 替代）


| 描述      | 内容                                       |
| ------- | ---------------------------------------- |
| 本质      | GPU 大模型推理引擎                              |
| 模型格式    | safetensors / GPTQ / AWQ                 |
| 支持训练权重  | ✔                                        |
| 推理速度    | 🚀 **行业最快**（比 HF Transformers 快 10～30 倍） |
| GPU 利用率 | 极高                                       |
| 适合场景    | 高并发，多用户访问 API                            |

**适用场景**

✔️ **服务器部署 LLM（API 服务）**
✔️ **多人同时访问做聊天机器人平台/知识库建模**
✔️ **需要最快推理速度、或 GPU 大量并发**
❌ 不适合开发板、CPU运行
❌ 不适合简单 Agent

---

| 对比项         | llama.cpp | Ollama  | vLLM                  |
| ----------- | --------- | ------- | --------------------- |
| 使用硬件        | CPU       | CPU/GPU | **GPU（推荐 A100/4090）** |
| 推理性能        | 慢        | ⚡ 中     | 🚀 **最快**             |
| 模型格式        | GGUF      | GGUF    | safetensors/GPTQ      |
| 是否支持Agent   | ❌         | **✔**   | ❌（但可结合框架）             |
| 是否适合开发板     | **✔ 最佳**  | 可，但体积较大 | ❌                     |
| 是否适合企业级 API | ❌         | ❌       | **✔ 最佳**              |
| 是否可本地运行     | ✔         | ✔       | 通常GPU服务器              |
| 是否支持函数调用    | ❌         | **✔**   | ❌                     |
| 是否支持高并发     | ❌         | ❌       | **✔**                 |

---



```
训练模型 (safetensors)
      │
      ├─→ gguf → llama.cpp (底层) → Ollama (用户用)
      └─→ GPTQ/AWQ/safetensors → vLLM (企业部署)
```



## 大模型常见文件格式分类

| 格式                          | 扩展名                        | 作用场景       | 优势            | 常见平台               |
| --------------------------- | -------------------------- | ---------- | ------------- | ------------------ |
| **PyTorch 原生格式**            | `.pt`, `.pth`, `.bin` (HF) | 训练 & 微调    | 灵活            | PyTorch / HF       |
| **SafeTensors**             | `.safetensors`             | 训练 & 推理    | 读写快、安全        | HF / PyTorch       |
| **TensorFlow**              | `.h5`, `.pb`, SavedModel   | 训练         | TensorFlow 专用 | TF/Keras           |
| **ONNX**                    | `.onnx`                    | 部署/跨平台     | 通用            | CPU/NPU/GPU部署      |
| **GGUF**                    | `.gguf`                    | 轻量推理       | 低内存/快速        | llama.cpp / Ollama |
| **GPTQ / AWQ / vLLM quant** | `.safetensors`, `.pt`      | 量化部署       | GPU友好         | vLLM, LMDeploy     |
| **MLX (Apple)**             | `.mlx`                     | M系列芯片      | 高效            | Apple MLX          |
| **Mistral-ish shard 格式**    | 多个 `.bin`                  | 分片加载       | 并行            | vLLM               |
| **TPU weights**             | 专有                         | Google TPU | TPU优化         | JAX                |



## 各大模型平台使用情况


| 框架 / 平台                      | 支持格式                                   |
| ---------------------------- | -------------------------------------- |
| **HuggingFace Transformers** | `.bin`, `.safetensors`, `.pt`, `.onnx` |
| **PyTorch**                  | `.pt`, `.pth`, `.safetensors`          |
| **TensorFlow/Keras**         | `.h5`, `.pb`                           |
| **llama.cpp**                | `.gguf`                                |
| **Ollama**                   | `.gguf`（推荐）或 `.safetensors`（需转换）       |
| **vLLM**                     | `.safetensors`, `.pt`, GPTQ/AWQ        |
| **LMDeploy**                 | `.bin`, `.safetensors`, AWQ            |
| **TensorRT-LLM**             | ONNX/engine                            |
| **OpenVINO / RK3588 / NPU**  | ONNX                                   |
| **Apple MLX**                | `.mlx`                                 |


## 格式特点对比

| 格式             | 是否量化 | 是否适合训练 | 是否适合 CPU / dev board | 是否可用于 Ollama   |
| -------------- | ---- | ------ | -------------------- | -------------- |
| `.bin`         | ❌    | ✔️     | ❌                    | ✔️（需转换）        |
| `.pt/.pth`     | ❌    | ✔️     | ❌                    | 不直接支持          |
| `.safetensors` | ❌    | ✔️     | ❌                    | 不直接支持（要转 gguf） |
| **`.gguf`**    | ✔️   | ❌      | ✔ **最优**             | ✔ **直接支持**     |
| `.onnx`        | 可 ✔  | ❌      | ✔ **常用于 NPU 部署**     | ❌              |
| GPTQ/AWQ       | ✔    | ❌      | ❌（主 GPU）             | ❌              |
| `.mlx`         | ✔    | ❌      | Apple 芯片             | ❌              |


